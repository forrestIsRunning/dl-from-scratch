# CNN (卷积神经网络) 示例

## 核心思想

**局部感受野 + 权重共享**

- **卷积层**：通过滑动窗口提取局部特征
- **池化层**：降维，增强平移不变性
- **全连接层**：最终分类

## 为什么需要 CNN？

传统 MLP 处理图像的问题：
1. 参数爆炸：28×28 图像 → 784×512 = 40万参数
2. 忽略空间结构：把图像拉平成向量，丢失邻居信息
3. 平移敏感：图像稍微移动，结果完全不同

CNN 的解决方案：
1. **局部连接**：每个神经元只看局部区域
2. **权重共享**：同一卷积核在整张图上滑动
3. **池化**：进一步降维，增强鲁棒性

## 快速开始

```bash
cd cnn
python train.py --model cnn --epochs 5
```

## 模型对比

| 模型 | 参数量 | 特点 |
|------|---------|------|
| MLP (baseline) | ~400K | 全连接，参数多 |
| CNN (本文) | ~28K | 卷积+池化，参数少 |
| LeNet-5 | ~60K | 经典架构 (1998) |

## 架构可视化

```
输入: [B, 1, 28, 28]
         ↓
    ┌─────────────────────────┐
    │  Conv2d(1→32)        │  卷积: 提取特征
    │  kernel=3x3           │
    │  ReLU                 │
    │  MaxPool2x2           │  池化: 降维
    └─────────────────────────┘
         ↓ [B, 32, 14, 14]
    ┌─────────────────────────┐
    │  Conv2d(32→64)       │
    │  kernel=3x3           │
    │  ReLU                 │
    │  MaxPool2x2           │
    └─────────────────────────┘
         ↓ [B, 64, 7, 7]
         Flatten
         ↓ [B, 3136]
    ┌─────────────────────────┐
    │  FC(3136→128)         │
    │  ReLU                 │
    │  FC(128→10)           │
    └─────────────────────────┘
         ↓ [B, 10]  (logits)
```

## 命令参数

```bash
python train.py --help

--model      模型类型 (cnn 或 lenet)
--epochs     训练轮数 (默认: 5)
--batch-size batch 大小 (默认: 64)
--lr         学习率 (默认: 0.01)
```

## 输出

训练完成后会生成：
- `cnn_cnn_best.pth` - 最佳模型权重
- `data/MNIST/` - MNIST 数据集（自动下载）
