# RNN/LSTM (循环神经网络) 示例

## 核心思想

**序列建模，保持隐藏状态**

通过循环结构处理变长序列，适合时间序列数据。

## 为什么需要 RNN？

传统 MLP 处理序列的问题：
1. 固定输入长度：无法处理变长序列
2. 忽略时序信息：把序列当独立样本
3. 参数爆炸：长序列需要大量参数

RNN 的解决方案：
1. **参数共享**：同一组权重处理每个时间步
2. **隐藏状态**：传递历史信息
3. **变长处理**：自然处理不同长度序列

## 快速开始

```bash
cd rnn
python train.py --model lstm --epochs 10
```

## 模型对比

| 模型 | 特点 | 适用场景 |
|------|------|----------|
| **RNN** | 基础循环，简单 | 短序列，简单任务 |
| **LSTM** | 细胞状态，门控 | 长序列，需要长期记忆 |
| **GRU** | LSTM 简化版 | 参数敏感，训练快 |
| **OptimizedLSTM** | PyTorch 内置 | 生产环境 |

## LSTM vs RNN vs GRU

### RNN (基础循环网络)

```
h_t = tanh(W_xh * x_t + W_hh * h_{t-1})
```

**问题**：
- 梯度消失：长序列梯度接近 0
- 梯度爆炸：长序列梯度无限增大
- 难以学习长期依赖

### LSTM (长短期记忆)

```
i_t = σ(W_xi * x_t + W_hi * h_{t-1})    # 输入门
f_t = σ(W_xf * x_t + W_hf * h_{t-1})    # 遗忘门
o_t = σ(W_xo * x_t + W_ho * h_{t-1})    # 输出门
C̃_t = tanh(W_xc * x_t + W_hc * h_{t-1}) # 候选状态

C_t = f_t ⊙ C_{t-1} + i_t ⊙ C̃_t        # 细胞状态
h_t = o_t ⊙ tanh(C_t)                    # 隐藏状态
```

**优势**：
- 细胞状态 C：信息"高速公路"，即使长时间也不消失
- 三重门控：精细控制信息流
- 解决梯度消失：恒等映射路径

### GRU (门控循环单元)

```
z_t = σ(W_xz * x_t + W_hz * h_{t-1})    # 更新门
r_t = σ(W_xr * x_t + W_hr * h_{t-1})    # 重置门
h̃_t = tanh(W_xh * x_t + r_t ⊙ W_hh * h_{t-1})

h_t = (1 - z_t) ⊙ h_{t-1} + z_t ⊙ h̃_t
```

**优势**：
- 比 LSTM 简单：少一个门状态
- 参数更少：训练更快
- 性能相当：大多数任务不差于 LSTM

## 架构可视化

```
LSTM 单步展开:

x_t ──┬──→ [W_xi] ──→ σ ──→ (×) ──┐
       │                              │  │
       ├──→ [W_xf] ──→ σ ──→ (×) ──┤ (+) → C_t → tanh → (×) ──→ h_t
       │                              │  │       │                │
       ├──→ [W_xc] ──→ tanh ──→ (×) ──┤        │                │
       │                                        │                │
       └──→ [W_xo] ──→ σ ─────────────────→ (×) ─────────┘

t-1 时刻:
h_{t-1} ──┬──→ [W_hi] ──────────────→ (+) ──┘
          │
          ├──→ [W_hf] ──→ (×) ──────────────┤
          │                                   │
          ├──→ [W_hc] ──→ (×) ──────────────┤
          │                                   │
          └──→ [W_ho] ────────────────────────┘

C_{t-1} ────────────────────────────────────→ (+)
```

## 命令参数

```bash
python train.py --help

--model       模型类型 (rnn/lstm/gru/optimized_lstm)
--epochs      训练轮数 (默认: 10)
--hidden-size 隐藏层大小 (默认: 32)
--lr          学习率 (默认: 0.001)
```

## 输出

训练完成后会生成：
- `rnn_{model}_best.pth` - 最佳模型权重
